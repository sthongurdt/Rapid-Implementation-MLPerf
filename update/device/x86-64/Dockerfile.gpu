# syntax=docker/dockerfile:1.7
FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    LANG=C.UTF-8 LC_ALL=C.UTF-8

# Paquetes base
RUN apt-get update && apt-get install -y --no-install-recommends \
      python3 python3-pip python3-dev \
      git build-essential cmake pkg-config \
      ca-certificates curl wget \
      libglib2.0-0 libsm6 libxext6 libxrender1 \
      && rm -rf /var/lib/apt/lists/*

# Aliases 'python' y 'pip'
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1 && \
    update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# Pip moderno
RUN python -m pip install --upgrade pip setuptools wheel

# PyTorch CUDA 12.4 (oficial)
RUN python -m pip install --index-url https://download.pytorch.org/whl/cu124 \
      torch torchvision

# ---- TensorFlow GPU 2.17 con CUDA/cuDNN vía wheels de NVIDIA ----
# Esto instala TF + librerías CUDA 12.x necesarias (cublas, cudnn, cufft, cusolver, cusparse, etc.)
RUN python -m pip install "tensorflow[and-cuda]==2.17.*"

# Exponer las libs de los wheels de NVIDIA en el runtime (ajusta la versión de Python si cambia)
ENV TF_CUDA_LIBS="/usr/local/lib/python3.10/dist-packages/nvidia/cuda_runtime/lib:\
/usr/local/lib/python3.10/dist-packages/nvidia/cublas/lib:\
/usr/local/lib/python3.10/dist-packages/nvidia/cudnn/lib:\
/usr/local/lib/python3.10/dist-packages/nvidia/cufft/lib:\
/usr/local/lib/python3.10/dist-packages/nvidia/cusolver/lib:\
/usr/local/lib/python3.10/dist-packages/nvidia/cusparse/lib"

# Asegura que TF y ORT encuentren tanto las libs de los wheels como las de /usr/local/cuda
ENV LD_LIBRARY_PATH=${TF_CUDA_LIBS}:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}

# --- ONNX Runtime GPU (CUDA 12.x) y utilidades ---
RUN python -m pip install \
      onnx==1.16.* onnxruntime-gpu==1.19.* \
      opencv-python-headless==4.10.* pillow==10.* \
      numpy==1.26.* pybind11==2.12.* \
      Cython==3.* future==1.* psutil==5.* rich==13.* \
      pycocotools==2.0.* --no-binary=pycocotools

# MLPerf LoadGen
RUN git clone --recursive https://github.com/mlcommons/inference /tmp/inference \
 && cd /tmp/inference/loadgen \
 && CFLAGS="-std=c++14" python setup.py install \
 && rm -rf /tmp/inference

# Verificaciones útiles (opcionales, no afectan a tus run_*.sh)
RUN python --version && pip --version && \
    python -c "import onnxruntime as ort; print('ORT providers:', ort.get_available_providers())" && \
    python -c "import tensorflow as tf; print('TF GPUs:', tf.config.list_physical_devices('GPU'))"

WORKDIR /root
ENTRYPOINT ["/bin/bash"]
